{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "74fd9b20-e765-474f-9beb-69e9e0c70d27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import tarfile\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets.utils import download_url\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as tt\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e348c3d1-4983-4531-900c-f5e1d152e939",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#Download data, normalize and apply transforms\n",
    "\n",
    "stats = ((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "\n",
    "train_tfms = tt.Compose([tt.RandomCrop(32, padding=4, padding_mode='reflect'),tt.RandomHorizontalFlip(),tt.ToTensor(),tt.Normalize(*stats,inplace=True)])\n",
    "valid_tfms = tt.Compose([tt.ToTensor(), tt.Normalize(*stats,inplace=True)])\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=train_tfms)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=valid_tfms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "87e0ecfc-36c2-4009-9b6c-593719eedfdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Prepare train and test data loader\n",
    "\n",
    "batch_size = 200\n",
    "train_dl = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=3,pin_memory=True)\n",
    "valid_dl = torch.utils.data.DataLoader(testset, batch_size=batch_size*2,\n",
    "                                         shuffle=False, num_workers=3,pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f7276e5d-ecfc-4703-93bf-87aff736ebec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#use cuda if available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e396ecee-59e7-4362-bdcb-14a1bf484daa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def conv_block(in_channels, out_channels, pool=False):\n",
    "    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1,bias=False), \n",
    "              nn.BatchNorm2d(out_channels), \n",
    "              nn.ReLU(inplace=True)]\n",
    "    if pool: layers.append(nn.MaxPool2d(2))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "#Adds a randomly sampled noise from a gaussian distribution to a standardized input tensor\n",
    "class AddNoise(nn.Module):\n",
    "    def __init__(self, mean=0, std=1):\n",
    "        super(AddNoise, self).__init__()\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "    \n",
    "    def to_device(self, device):\n",
    "        # Move the internal state (mean and std) to the specified device\n",
    "        self.mean = torch.tensor(self.mean, device=device)\n",
    "        self.std = torch.tensor(self.std, device=device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        noise = torch.randn(x.size(),device=device) * self.std + self.mean\n",
    "        return x+noise\n",
    "\n",
    "#Resnet 9 architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,in_channels,num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=3, padding=1,bias = False)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(64)\n",
    "        self.add_noise = AddNoise()\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1,bias=False)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(128)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.res1 = nn.Sequential(conv_block(128, 128),conv_block(128, 128))\n",
    "        self.res1conv1 = nn.Conv2d(128, 128, kernel_size=3, padding=1,bias=False)\n",
    "        self.res1batchnorm1 = nn.BatchNorm2d(128)\n",
    "        self.res1conv2 = nn.Conv2d(128, 128, kernel_size=3, padding=1,bias=False)\n",
    "        self.res1batchnorm2 = nn.BatchNorm2d(128)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.batchnorm3 = nn.BatchNorm2d(256)\n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1,bias=False)\n",
    "        self.batchnorm4 = nn.BatchNorm2d(512)\n",
    "        self.res2 = nn.Sequential(conv_block(512, 512),conv_block(512, 512))\n",
    "        self.res2conv1 = nn.Conv2d(512, 512, kernel_size=3, padding=1,bias=False)\n",
    "        self.res2batchnorm1 = nn.BatchNorm2d(512)\n",
    "        self.res2conv2 = nn.Conv2d(512, 512, kernel_size=3, padding=1,bias=False)\n",
    "        self.res2batchnorm2 = nn.BatchNorm2d(512)\n",
    "        self.classifier = nn.Sequential(nn.MaxPool2d(4), \n",
    "                                        nn.Flatten(), \n",
    "                                        nn.Linear(512, num_classes))       \n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.batchnorm1(self.conv1(x)))\n",
    "        x = (x-torch.mean(x))/(torch.std(x))  \n",
    "        x = self.pool(F.relu(self.batchnorm2(self.conv2(x))))\n",
    "        x = (x-torch.mean(x))/(torch.std(x))\n",
    "        y = x\n",
    "        x = F.relu(self.res1batchnorm1(self.res1conv1(x)))\n",
    "        x = (x-torch.mean(x))/(torch.std(x))\n",
    "        x = F.relu(self.res1batchnorm2(self.res1conv2(x)))\n",
    "        x = (x-torch.mean(x))/(torch.std(x))\n",
    "        x = x+y\n",
    "        x = self.pool(F.relu(self.batchnorm3(self.conv3(x))))\n",
    "        x = (x-torch.mean(x))/(torch.std(x))\n",
    "        x = self.pool(F.relu(self.batchnorm4(self.conv4(x))))\n",
    "        x = (x-torch.mean(x))/(torch.std(x))\n",
    "        z = x\n",
    "        x = F.relu(self.res2batchnorm1(self.res2conv1(x)))\n",
    "        x = (x-torch.mean(x))/(torch.std(x))\n",
    "        x = F.relu(self.res2batchnorm2(self.res2conv2(x)))\n",
    "        x = (x-torch.mean(x))/(torch.std(x))\n",
    "        x = x+z\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    #Adding noise during inference to model analog hardware\n",
    "    def inference_with_noise(self, x):\n",
    "        x = F.relu(self.batchnorm1(self.conv1(x)))\n",
    "        x = (x-torch.mean(x))/(torch.std(x))\n",
    "        x = self.add_noise(x)\n",
    "        x = self.pool(F.relu(self.batchnorm2(self.conv2(x))))\n",
    "        x = (x-torch.mean(x))/(torch.std(x))\n",
    "        x = self.add_noise(x)\n",
    "        y = x\n",
    "        x = F.relu(self.res1batchnorm1(self.res1conv1(x)))\n",
    "        x = (x-torch.mean(x))/(torch.std(x))\n",
    "        x = self.add_noise(x)\n",
    "        x = F.relu(self.res1batchnorm2(self.res1conv2(x)))\n",
    "        x = (x-torch.mean(x))/(torch.std(x))\n",
    "        x = self.add_noise(x)\n",
    "        x = x+y\n",
    "        x = self.pool(F.relu(self.batchnorm3(self.conv3(x))))\n",
    "        x = (x-torch.mean(x))/(torch.std(x))\n",
    "        x = self.add_noise(x)\n",
    "        x = self.pool(F.relu(self.batchnorm4(self.conv4(x))))\n",
    "        x = (x-torch.mean(x))/(torch.std(x))\n",
    "        x = self.add_noise(x)\n",
    "        z = x\n",
    "        x = F.relu(self.res2batchnorm1(self.res2conv1(x)))\n",
    "        x = (x-torch.mean(x))/(torch.std(x))\n",
    "        x = self.add_noise(x)\n",
    "        x = F.relu(self.res2batchnorm2(self.res2conv2(x)))\n",
    "        x = (x-torch.mean(x))/(torch.std(x))\n",
    "        x = self.add_noise(x)\n",
    "        x = x+z\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "net = Net(3,10)\n",
    "net = net.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "110eaaec-7962-4663-aba4-2fb400d6b5a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "max_lr = 0.005\n",
    "grad_clip = 0.1\n",
    "weight_decay = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9c8f652a-16e3-44ca-bc19-628133a10984",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "#We use one cycle LR and Adam optimizer with weight decay \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=max_lr, weight_decay = weight_decay)\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, \n",
    "                                                steps_per_epoch=len(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6bd08683-c75c-453c-9f40-b2880d4453af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    50] loss: 1.703 learning rate: 0.0002056198493911067 training accuracy: 41.720\n",
      "[1,   100] loss: 1.306 learning rate: 0.00022291290839822342 training accuracy: 47.245\n",
      "[1,   150] loss: 1.182 learning rate: 0.0002517971936100531 training accuracy: 50.820\n",
      "[1,   200] loss: 1.089 learning rate: 0.0002921318585742995 training accuracy: 53.495\n",
      "[1,   250] loss: 0.995 learning rate: 0.00034372022213984173 training accuracy: 55.864\n",
      "[2,    50] loss: 0.926 learning rate: 0.00040631072751949604 training accuracy: 66.790\n",
      "[2,   100] loss: 0.904 learning rate: 0.0004795981689390662 training accuracy: 67.650\n",
      "[2,   150] loss: 0.816 learning rate: 0.0005632251798912595 training accuracy: 68.880\n",
      "[2,   200] loss: 0.798 learning rate: 0.0006567839757373895 training accuracy: 69.733\n",
      "[2,   250] loss: 0.765 learning rate: 0.0007598183421595539 training accuracy: 70.518\n",
      "[3,    50] loss: 0.709 learning rate: 0.0008718258597671131 training accuracy: 75.300\n",
      "[3,   100] loss: 0.700 learning rate: 0.000992260354009792 training accuracy: 75.700\n",
      "[3,   150] loss: 0.693 learning rate: 0.0011205345584510578 training accuracy: 75.690\n",
      "[3,   200] loss: 0.657 learning rate: 0.0012560229784150268 training accuracy: 76.080\n",
      "[3,   250] loss: 0.668 learning rate: 0.0013980649410431098 training accuracy: 76.230\n",
      "[4,    50] loss: 0.611 learning rate: 0.0015459678168876106 training accuracy: 78.430\n",
      "[4,   100] loss: 0.605 learning rate: 0.0016990103973330358 training accuracy: 78.975\n",
      "[4,   150] loss: 0.630 learning rate: 0.0018564464113760373 training accuracy: 78.943\n",
      "[4,   200] loss: 0.631 learning rate: 0.00201750816461535 training accuracy: 78.907\n",
      "[4,   250] loss: 0.580 learning rate: 0.002181410282707173 training accuracy: 79.084\n",
      "[5,    50] loss: 0.523 learning rate: 0.0023473535410320364 training accuracy: 81.790\n",
      "[5,   100] loss: 0.602 learning rate: 0.002514528761898813 training accuracy: 80.830\n",
      "[5,   150] loss: 0.572 learning rate: 0.002682120760282209 training accuracy: 80.810\n",
      "[5,   200] loss: 0.575 learning rate: 0.0028493123188534086 training accuracy: 80.663\n",
      "[5,   250] loss: 0.567 learning rate: 0.0030152881729206965 training accuracy: 80.702\n",
      "[6,    50] loss: 0.545 learning rate: 0.0031792389858485703 training accuracy: 81.390\n",
      "[6,   100] loss: 0.508 learning rate: 0.0033403652955702797 training accuracy: 82.035\n",
      "[6,   150] loss: 0.556 learning rate: 0.0034978814129496927 training accuracy: 81.767\n",
      "[6,   200] loss: 0.513 learning rate: 0.0036510192529831545 training accuracy: 81.995\n",
      "[6,   250] loss: 0.573 learning rate: 0.0037990320801595478 training accuracy: 81.864\n",
      "[7,    50] loss: 0.482 learning rate: 0.003941198149715263 training accuracy: 83.640\n",
      "[7,   100] loss: 0.574 learning rate: 0.004076824227028532 training accuracy: 82.185\n",
      "[7,   150] loss: 0.556 learning rate: 0.004205248967991708 training accuracy: 81.990\n",
      "[7,   200] loss: 0.563 learning rate: 0.004325846143878025 training accuracy: 81.793\n",
      "[7,   250] loss: 0.546 learning rate: 0.0044380276949776455 training accuracy: 81.914\n",
      "[8,    50] loss: 0.504 learning rate: 0.0045412465981127526 training accuracy: 83.310\n",
      "[8,   100] loss: 0.531 learning rate: 0.004634999534049035 training accuracy: 82.950\n",
      "[8,   150] loss: 0.515 learning rate: 0.004718829341796624 training accuracy: 82.990\n",
      "[8,   200] loss: 0.489 learning rate: 0.004792327247832779 training accuracy: 83.178\n",
      "[8,   250] loss: 0.486 learning rate: 0.00485513485937608 training accuracy: 83.364\n",
      "[9,    50] loss: 0.493 learning rate: 0.004906945911992486 training accuracy: 83.500\n",
      "[9,   100] loss: 0.489 learning rate: 0.004947507763011517 training accuracy: 83.675\n",
      "[9,   150] loss: 0.549 learning rate: 0.0049766226234703274 training accuracy: 83.370\n",
      "[9,   200] loss: 0.505 learning rate: 0.004994148522578412 training accuracy: 83.567\n",
      "[9,   250] loss: 0.548 learning rate: 0.005 training accuracy: 83.334\n",
      "[10,    50] loss: 0.449 learning rate: 0.004998881085184435 training accuracy: 84.930\n",
      "[10,   100] loss: 0.423 learning rate: 0.00499552534231804 training accuracy: 85.275\n",
      "[10,   150] loss: 0.476 learning rate: 0.004989935775245158 training accuracy: 85.053\n",
      "[10,   200] loss: 0.507 learning rate: 0.00498211738738533 training accuracy: 84.562\n",
      "[10,   250] loss: 0.416 learning rate: 0.004972077177254559 training accuracy: 84.864\n",
      "[11,    50] loss: 0.378 learning rate: 0.004959824132200689 training accuracy: 87.310\n",
      "[11,   100] loss: 0.382 learning rate: 0.004945369220358507 training accuracy: 87.140\n",
      "[11,   150] loss: 0.439 learning rate: 0.0049287253808317855 training accuracy: 86.603\n",
      "[11,   200] loss: 0.461 learning rate: 0.004909907512111026 training accuracy: 86.182\n",
      "[11,   250] loss: 0.463 learning rate: 0.004888932458737294 training accuracy: 85.950\n",
      "[12,    50] loss: 0.414 learning rate: 0.004865818996224072 training accuracy: 86.460\n",
      "[12,   100] loss: 0.410 learning rate: 0.004840587814250637 training accuracy: 86.535\n",
      "[12,   150] loss: 0.418 learning rate: 0.004813261498141987 training accuracy: 86.393\n",
      "[12,   200] loss: 0.424 learning rate: 0.004783864508651926 training accuracy: 86.330\n",
      "[12,   250] loss: 0.433 learning rate: 0.004752423160067369 training accuracy: 86.250\n",
      "[13,    50] loss: 0.376 learning rate: 0.004718965596653495 training accuracy: 87.340\n",
      "[13,   100] loss: 0.392 learning rate: 0.004683521767460818 training accuracy: 87.325\n",
      "[13,   150] loss: 0.412 learning rate: 0.004646123399516729 training accuracy: 87.073\n",
      "[13,   200] loss: 0.423 learning rate: 0.004606803969425512 training accuracy: 86.778\n",
      "[13,   250] loss: 0.386 learning rate: 0.004565598673402244 training accuracy: 86.856\n",
      "[14,    50] loss: 0.366 learning rate: 0.004522544395767425 training accuracy: 87.950\n",
      "[14,   100] loss: 0.366 learning rate: 0.004477679675930507 training accuracy: 87.945\n",
      "[14,   150] loss: 0.432 learning rate: 0.004431044673891907 training accuracy: 87.357\n",
      "[14,   200] loss: 0.424 learning rate: 0.0043826811342943675 training accuracy: 86.968\n",
      "[14,   250] loss: 0.404 learning rate: 0.004332632349055847 training accuracy: 86.948\n",
      "[15,    50] loss: 0.362 learning rate: 0.0042809431186173925 training accuracy: 87.630\n",
      "[15,   100] loss: 0.357 learning rate: 0.004227659711840672 training accuracy: 87.950\n",
      "[15,   150] loss: 0.336 learning rate: 0.004172829824591082 training accuracy: 88.183\n",
      "[15,   200] loss: 0.410 learning rate: 0.004116502537043487 training accuracy: 87.828\n",
      "[15,   250] loss: 0.380 learning rate: 0.0040587282697488155 training accuracy: 87.692\n",
      "[16,    50] loss: 0.344 learning rate: 0.003999558738500837 training accuracy: 88.690\n",
      "[16,   100] loss: 0.356 learning rate: 0.003939046908043527 training accuracy: 88.360\n",
      "[16,   150] loss: 0.322 learning rate: 0.003877246944660442 training accuracy: 88.643\n",
      "[16,   200] loss: 0.328 learning rate: 0.0038142141676885634 training accuracy: 88.728\n",
      "[16,   250] loss: 0.377 learning rate: 0.003750005 training accuracy: 88.454\n",
      "[17,    50] loss: 0.297 learning rate: 0.003684676917495872 training accuracy: 89.910\n",
      "[17,   100] loss: 0.320 learning rate: 0.003618288397657598 training accuracy: 89.575\n",
      "[17,   150] loss: 0.312 learning rate: 0.003550898867201631 training accuracy: 89.587\n",
      "[17,   200] loss: 0.350 learning rate: 0.0034825686488844927 training accuracy: 89.237\n",
      "[17,   250] loss: 0.350 learning rate: 0.0034133589075057446 training accuracy: 88.982\n",
      "[18,    50] loss: 0.284 learning rate: 0.0033433315951572015 training accuracy: 90.700\n",
      "[18,   100] loss: 0.299 learning rate: 0.003272549395767425 training accuracy: 90.380\n",
      "[18,   150] loss: 0.299 learning rate: 0.0032010756689911136 training accuracy: 90.200\n",
      "[18,   200] loss: 0.328 learning rate: 0.003128974393493634 training accuracy: 89.882\n",
      "[18,   250] loss: 0.314 learning rate: 0.0030563101096814462 training accuracy: 89.790\n",
      "[19,    50] loss: 0.270 learning rate: 0.0029831478619297054 training accuracy: 91.140\n",
      "[19,   100] loss: 0.252 learning rate: 0.00290955314035873 training accuracy: 91.170\n",
      "[19,   150] loss: 0.267 learning rate: 0.0028355918222114805 training accuracy: 91.007\n",
      "[19,   200] loss: 0.289 learning rate: 0.002761330112884501 training accuracy: 90.817\n",
      "[19,   250] loss: 0.269 learning rate: 0.0026868344866651252 training accuracy: 90.828\n",
      "[20,    50] loss: 0.247 learning rate: 0.002612171627227984 training accuracy: 91.230\n",
      "[20,   100] loss: 0.235 learning rate: 0.002537408367944089 training accuracy: 91.515\n",
      "[20,   150] loss: 0.263 learning rate: 0.0024626116320559115 training accuracy: 91.273\n",
      "[20,   200] loss: 0.272 learning rate: 0.002387848372772017 training accuracy: 91.145\n",
      "[20,   250] loss: 0.257 learning rate: 0.002313185513334875 training accuracy: 91.162\n",
      "[21,    50] loss: 0.214 learning rate: 0.0022386898871154994 training accuracy: 93.010\n",
      "[21,   100] loss: 0.203 learning rate: 0.0021644281777885204 training accuracy: 92.895\n",
      "[21,   150] loss: 0.229 learning rate: 0.00209046685964127 training accuracy: 92.593\n",
      "[21,   200] loss: 0.224 learning rate: 0.002016872138070295 training accuracy: 92.468\n",
      "[21,   250] loss: 0.240 learning rate: 0.001943709890318554 training accuracy: 92.340\n",
      "[22,    50] loss: 0.182 learning rate: 0.0018710456065063664 training accuracy: 93.990\n",
      "[22,   100] loss: 0.175 learning rate: 0.0017989443310088866 training accuracy: 93.960\n",
      "[22,   150] loss: 0.180 learning rate: 0.0017274706042325755 training accuracy: 93.923\n",
      "[22,   200] loss: 0.206 learning rate: 0.0016566884048427992 training accuracy: 93.650\n",
      "[22,   250] loss: 0.176 learning rate: 0.0015866610924942562 training accuracy: 93.658\n",
      "[23,    50] loss: 0.153 learning rate: 0.001517451351115508 training accuracy: 94.460\n",
      "[23,   100] loss: 0.144 learning rate: 0.00144912113279837 training accuracy: 94.630\n",
      "[23,   150] loss: 0.149 learning rate: 0.0013817316023424014 training accuracy: 94.710\n",
      "[23,   200] loss: 0.141 learning rate: 0.0013153430825041284 training accuracy: 94.748\n",
      "[23,   250] loss: 0.153 learning rate: 0.0012500150000000008 training accuracy: 94.722\n",
      "[24,    50] loss: 0.113 learning rate: 0.0011858058323114362 training accuracy: 96.120\n",
      "[24,   100] loss: 0.122 learning rate: 0.0011227730553395585 training accuracy: 96.070\n",
      "[24,   150] loss: 0.117 learning rate: 0.0010609730919564735 training accuracy: 95.943\n",
      "[24,   200] loss: 0.141 learning rate: 0.0010004612614991628 training accuracy: 95.700\n",
      "[24,   250] loss: 0.114 learning rate: 0.000941291730251185 training accuracy: 95.770\n",
      "[25,    50] loss: 0.093 learning rate: 0.0008835174629565134 training accuracy: 96.610\n",
      "[25,   100] loss: 0.091 learning rate: 0.0008271901754089188 training accuracy: 96.780\n",
      "[25,   150] loss: 0.084 learning rate: 0.0007723602881593283 training accuracy: 96.893\n",
      "[25,   200] loss: 0.088 learning rate: 0.000719076881382608 training accuracy: 96.852\n",
      "[25,   250] loss: 0.094 learning rate: 0.000667387650944153 training accuracy: 96.840\n",
      "[26,    50] loss: 0.063 learning rate: 0.0006173388657056329 training accuracy: 97.810\n",
      "[26,   100] loss: 0.063 learning rate: 0.0005689753261080938 training accuracy: 97.825\n",
      "[26,   150] loss: 0.067 learning rate: 0.0005223403240694942 training accuracy: 97.780\n",
      "[26,   200] loss: 0.064 learning rate: 0.0004774756042325754 training accuracy: 97.793\n",
      "[26,   250] loss: 0.062 learning rate: 0.0004344213265977565 training accuracy: 97.836\n",
      "[27,    50] loss: 0.051 learning rate: 0.00039321603057448863 training accuracy: 98.490\n",
      "[27,   100] loss: 0.050 learning rate: 0.0003538966004832708 training accuracy: 98.415\n",
      "[27,   150] loss: 0.047 learning rate: 0.00031649823253918205 training accuracy: 98.457\n",
      "[27,   200] loss: 0.048 learning rate: 0.00028105440334650483 training accuracy: 98.487\n",
      "[27,   250] loss: 0.049 learning rate: 0.0002475968399326314 training accuracy: 98.454\n",
      "[28,    50] loss: 0.036 learning rate: 0.00021615549134807398 training accuracy: 98.900\n",
      "[28,   100] loss: 0.037 learning rate: 0.000186758501858013 training accuracy: 98.925\n",
      "[28,   150] loss: 0.034 learning rate: 0.0001594321857493634 training accuracy: 98.933\n",
      "[28,   200] loss: 0.035 learning rate: 0.00013420100377592735 training accuracy: 98.915\n",
      "[28,   250] loss: 0.040 learning rate: 0.0001110875412627059 training accuracy: 98.884\n",
      "[29,    50] loss: 0.029 learning rate: 9.011248788897386e-05 training accuracy: 99.240\n",
      "[29,   100] loss: 0.028 learning rate: 7.129461916821416e-05 training accuracy: 99.270\n",
      "[29,   150] loss: 0.032 learning rate: 5.465077964149312e-05 training accuracy: 99.233\n",
      "[29,   200] loss: 0.030 learning rate: 4.0195867799311825e-05 training accuracy: 99.220\n",
      "[29,   250] loss: 0.029 learning rate: 2.7942822745440944e-05 training accuracy: 99.212\n",
      "[30,    50] loss: 0.026 learning rate: 1.790261261467003e-05 training accuracy: 99.370\n",
      "[30,   100] loss: 0.027 learning rate: 1.0084224754842283e-05 training accuracy: 99.305\n",
      "[30,   150] loss: 0.026 learning rate: 4.4946576819597904e-06 training accuracy: 99.317\n",
      "[30,   200] loss: 0.026 learning rate: 1.1389148155645285e-06 training accuracy: 99.290\n",
      "[30,   250] loss: 0.028 learning rate: 2e-08 training accuracy: 99.278\n",
      "Finished Training, Training time: 280.03139808299966\n"
     ]
    }
   ],
   "source": [
    "#Function to get learning rate during training\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "    \n",
    "training_time = 0\n",
    "\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "    \n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    lrs = []\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    for i, data in enumerate(train_dl, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_value_(net.parameters(), grad_clip)\n",
    "        optimizer.step()\n",
    "        lrs.append(get_lr(optimizer))\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Calculate training accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "        training_acc = (correct_predictions/total_predictions)*100\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 50 == 49:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 50:.3f} learning rate: {lrs[i]} training accuracy: {training_acc:.3f}')\n",
    "            running_loss = 0.0\n",
    "            \n",
    "    training_time += time.perf_counter()-start_time\n",
    "\n",
    "print(f'Finished Training, Training time: {training_time}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b9fc7962-fda2-4cc8-9864-36766a6e4433",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Saving the weights for initialising the weights in distillation learning\n",
    "\n",
    "torch.save(net.state_dict(), 'teacher_30epochs_pre.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a9f387-12b2-440b-8226-6b76bb4ad741",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in valid_dl:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "        #Inferencing on clean hardware\n",
    "        outputs = net(images) \n",
    "            \n",
    "        #Inferencing on noisy hardware\n",
    "        #outputs = net.inference_with_noise(images) \n",
    "\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f} %')    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
